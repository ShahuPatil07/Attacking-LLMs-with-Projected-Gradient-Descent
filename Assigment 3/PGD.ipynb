{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NlHOs3ZFk4-E"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "class Dummy:\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "        self.vocab = self.tokenizer.get_vocab()  # This provides the vocabulary\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Assuming x is a list of strings for simplicity\n",
        "        inputs = self.tokenizer(x, return_tensors='pt', padding=True, truncation=True)\n",
        "        outputs = self.model(**inputs)\n",
        "        return outputs.logits"
      ],
      "metadata": {
        "id": "cBRu8iALBecM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplex Projection\n",
        "def simplex_projection(s):\n",
        "    s_sorted = np.sort(s)\n",
        "    s_cumsum = np.cumsum(s_sorted)\n",
        "    rho = np.argmax(s_sorted > (s_cumsum - 1) / np.arange(1, len(s) + 1))\n",
        "    theta = (s_cumsum[rho] - 1) / (rho + 1)\n",
        "    return np.maximum(s - theta, 0)\n",
        "\n",
        "# Entropy Projection\n",
        "def entropy_projection(s, target_entropy=2):\n",
        "    mask = s > 0\n",
        "    center = mask / np.sum(mask)\n",
        "    R = np.sqrt(np.maximum(0, 1 - target_entropy - 1 / np.sum(mask)))\n",
        "    if R >= np.linalg.norm(s - center):\n",
        "        return s\n",
        "    else:\n",
        "        return simplex_projection(R / np.linalg.norm(s - center) * (s - center) + center)\n"
      ],
      "metadata": {
        "id": "sg8Q4p4j81dd"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relaxed_one_hot(prompt, tokenizer):\n",
        "    # Tokenize the string prompt\n",
        "    token_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    # Create one-hot encoding\n",
        "    relaxed = np.zeros((len(token_ids), vocab_size))\n",
        "    for i, token_id in enumerate(token_ids):\n",
        "        relaxed[i, token_id] = 1\n",
        "\n",
        "    # Add a small noise\n",
        "    return relaxed + np.random.uniform(0, 1, relaxed.shape) * 0.1"
      ],
      "metadata": {
        "id": "ZK7V65Fr-F9i"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusted PGD Attack with debugging prints\n",
        "def pgd_attack(model, original_prompt, loss_fn, learning_rate, epochs):\n",
        "    X_t = relaxed_one_hot(original_prompt, model.tokenizer)  # Pass the tokenizer\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_prompt = None\n",
        "\n",
        "    X_t_tensor = torch.tensor(X_t, requires_grad=True).float()\n",
        "    X_t_t2= X_t_tensor.detach()\n",
        "    optimizer = torch.optim.Adam([X_t_t2], lr=learning_rate)\n",
        "\n",
        "    for t in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        decoded_prompts = [model.tokenizer.decode([torch.argmax(X_t_tensor[i]).item()]) for i in range(X_t_tensor.shape[0])]\n",
        "        logits = model(decoded_prompts)\n",
        "        target = torch.tensor([1] * logits.shape[0], dtype=torch.long)  # Example target tensor with the same batch size\n",
        "        loss = loss_fn(logits, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            X_t = X_t_tensor.detach().numpy()  # Detach the tensor to get a new numpy array for the next iteration\n",
        "            for i in range(X_t.shape[0]):\n",
        "                X_t[i] = simplex_projection(X_t[i])\n",
        "                X_t[i] = entropy_projection(X_t[i])\n",
        "\n",
        "        # Update X_t_tensor with the projected values\n",
        "        X_t_tensor = torch.tensor(X_t, requires_grad=True).float()\n",
        "\n",
        "        decoded_prompts = [model.tokenizer.decode([torch.argmax(X_t_tensor[i]).item()]) for i in range(X_t_tensor.shape[0])]\n",
        "        logits = model(decoded_prompts)\n",
        "        current_loss = loss_fn(logits, target)\n",
        "        if current_loss < best_loss:\n",
        "            best_loss = current_loss\n",
        "            best_prompt = X_t\n",
        "\n",
        "    return best_prompt\n",
        "\n",
        "# Example Usage with debugging prints in projection functions\n",
        "model = Dummy()\n",
        "loss_fn = lambda logits, target: F.cross_entropy(logits, target)  # Example loss function\n",
        "\n",
        "original_prompt = \"example prompt for PGD attack\"\n",
        "learning_rate = 0.001  # Adjust learning rate\n",
        "epochs = 10\n",
        "\n",
        "# Perform PGD attack\n",
        "adversarial_prompt = pgd_attack(model, original_prompt, loss_fn, learning_rate, epochs)\n",
        "print(\"Adversarial Prompt:\", adversarial_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzXYhQX7-glG",
        "outputId": "261056d3-ae08-4cbc-df78-c957e77d6a89"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial Prompt: [[1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KyL8sBMXCBci"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}