The code sets up a scenario where we use a pre-trained language model, specifically BERT (Bidirectional Encoder Representations from Transformers), to perform a Projected Gradient Descent (PGD) attack on a given prompt or initial text sequence. The objective is to perturb the prompt in such a way that the model's prediction significantly changes, showcasing vulnerability to adversarial examples.

* BERT is set up to understand text, and we start with a simple sentence (prompt).  The code tries small changes to this sentence to see if BERT's prediction changes a lot. It checks how much BERT's understanding changes with each tweak.  This process repeats several times, each time trying to find the best tweak that fools BERT the most.

* Constraints: There are rules (like simplex_projection and entropy_projection) to make sure the changes to the text are sensible and don't make it too hard for BERT to understand. these rules are after replaxed_one_hot_necoding is applied.
* Initially this notebook threw a lot of errors. I kept debugging and clearing it. I'll be honest- the first draft was after few suggestions by ChatGPT for a dummy model, but then i made few changes to takle errors one by one. Finally i'm getting array of all ones for some reason. I think when I detached X_T_tensor to make it a leaf tensor from non-leaf, there is a possiblilty of some error. Still trying to debug, but due to deadline and other commitments, I'm laggiing a bit behind for this. Thanks. 
